@article{behnelCythonBestBoth2011,
  title = {Cython: {{The Best}} of {{Both Worlds}}},
  author = {Behnel, S and Bradshaw, R and Citro, C and Dalcin, L and Seljebotn, D S and Smith, K},
  year = {2011},
  month = mar,
  journal = {Computing in Science Engineering},
  volume = {13},
  number = {2},
  pages = {31--39},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2010.118},
  abstract = {Cython is a Python language extension that allows explicit type declarations and is compiled directly to C. As such, it addresses Python's large overhead for numerical loops and the difficulty of efficiently using existing C and Fortran code, which Cython can interact with natively.},
  keywords = {C language,Cython,Cython language,Fortran code,numerical analysis,numerical loops,numerics,programming language,Python,Python language extension,scientific computing}
}

@article{bottouCounterfactualReasoningLearning2013,
  title = {Counterfactual {{Reasoning}} and {{Learning Systems}}: {{The Example}} of {{Computational Advertising}}},
  author = {Bottou, L{\'e}on and Peters, Jonas and {Qui{\~n}onero-Candela}, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {1},
  pages = {3207--3260},
  issn = {1532-4435},
  url = {http://www.jmlr.org/papers/volume14/bottou13a/bottou13a.pdf},
  abstract = {Abstract This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.}
}

@unpublished{buitinckAPIDesignMachine2013a,
  title = {{{API Design}} for {{Machine Learning Software}}: {{Experiences}} from the Scikit-Learn {{Project}}},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"e}l},
  year = {2013},
  month = sep,
  url = {http://arxiv.org/abs/1309.0238},
  abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
  keywords = {LensKit References}
}

@inproceedings{caoMakingSystemsForget2015,
  title = {Towards {{Making Systems Forget}} with {{Machine Unlearning}}},
  booktitle = {Proceedings of the 36th {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Cao, Yinzhi and Yang, Junfeng},
  year = {2015},
  month = may,
  publisher = {IEEE},
  url = {http://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf},
  abstract = {Today's systems produce a wealth of data every day, and the data further generates more data, i.e., the derived data, forming into a complex data propagation network, defined as the data's lineage. There are many reasons for users and administrators to forget certain data including the data's lineage. From the privacy perspective, a system may leak private information of certain users, and those users unhappy about privacy leaks naturally want to forget their data and its lineage. From the security perspective, an anomaly detection system can be polluted by adversaries through injecting manually crafted data into the training set. Therefore, we envision forgetting systems, capable of completely forgetting certain data and its lineage. In this paper, we focus on making learning systems forget, the process of which is defined as machine unlearning or unlearning. To perform unlearning upon learning system, we present general unlearning criteria, i.e., converting a learning system or part of it into a summation form of statistical query learning model, and updating all the summations to achieve unlearning. Then, we integrate our unlearning criteria into an unlearning architecture that interacts with all the components of a learning system, such as sample clustering and feature selection. To demonstrate our unlearning criteria and architecture, we select four real-world learning systems, including an item-item recommendation system, an online social network spam filter, and a malware detection system. These systems are first exposed to an adversarial environment, e.g., if the system is potentially vulnerable to training data pollution, we first pollute the training data set and show that the detection rate drops significantly. Then, we apply our unlearning technique upon those affected systems, either polluted or leaking private information. Our results show that after unlearning, the detection rate of a polluted system increases back to the one before pollution, and a system leaking a particular user's private information completely forgets that information.},
  keywords = {Research Using LensKit,Zotero Import (Mar 30),Zotero Import (Mar 30)/Group Libraries/LensKit}
}

@inproceedings{carvalhoFAiRFrameworkAnalyses2018,
  title = {{{FAiR}}: {{A Framework}} for {{Analyses}} and {{Evaluations}} on {{Recommender Systems}}},
  booktitle = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2018},
  author = {Carvalho, Diego and Silva, N{\'i}collas and Silveira, Thiago and Mour{\~a}o, Fernando and Pereira, Adriano and Dias, Diego and Rocha, Leonardo},
  year = {2018},
  pages = {383--397},
  publisher = {Springer International Publishing},
  doi = {10.1007/978-3-319-95168-3_26},
  abstract = {Recommender systems (RSs) have become essential tools in e-commerce applications, helping users in the decision-making process. Evaluation on these tools is, however, a major divergence point nowadays, since there is no consensus regarding which metrics are necessary to consolidate new RSs. For this reason, distinct frameworks have been developed to ease the deployment of RSs in research and/or production environments. In the present work, we perform an extensive study of the most popular evaluation metrics, organizing them into three groups: Effectiveness-based, Complementary Dimensions of Quality and Domain Profiling. Further, we consolidate a framework named FAiR to help researchers in evaluating their RSs using these metrics, besides identifying the characteristics of data collections that may intrinsically affect RSs performance. FAiR is compatible with the output format of the main existing RSs libraries (i.e., MyMediaLite and LensKit).},
  keywords = {Research Using LensKit}
}

@article{dacremaTroublingAnalysisReproducibility2021,
  title = {A {{Troubling Analysis}} of {{Reproducibility}} and {{Progress}} in {{Recommender Systems Research}}},
  author = {Dacrema, Maurizio Ferrari and Boglio, Simone and Cremonesi, Paolo and Jannach, Dietmar},
  year = {2021},
  month = jan,
  journal = {ACM Trans. Inf. Syst. Secur.},
  volume = {39},
  number = {2},
  pages = {1--49},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1094-9224},
  doi = {10.1145/3434185},
  abstract = {The design of algorithms that generate personalized ranked item lists is a central topic of research in the field of recommender systems. In the past few years, in particular, approaches based on deep learning (neural) techniques have become dominant in the literature. For all of them, substantial progress over the state-of-the-art is claimed. However, indications exist of certain problems in today's research practice, e.g., with respect to the choice and optimization of the baselines used for comparison, raising questions about the published claims. To obtain a better understanding of the actual progress, we have compared recent results in the area of neural recommendation approaches based on collaborative filtering against a consistent set of existing simple baselines. The worrying outcome of the analysis of these recent works---all were published at prestigious scientific conferences between 2015 and 2018---is that 11 of the 12 reproducible neural approaches can be outperformed by conceptually simple methods, e.g., based on the nearest-neighbor heuristic or linear models. None of the computationally complex neural methods was actually consistently better than already existing learning-based techniques, e.g., using matrix factorization or linear models. In our analysis, we discuss common issues in today's research practice, which, despite the many papers that are published on the topic, have apparently led the field to a certain level of stagnation.1},
  keywords = {evaluation,LensKit References,reproducibility Recommender systems deep learning}
}

@article{deshpande:iknn,
  title = {Item-Based Top-{{N Recommendation Algorithms}}},
  author = {Deshpande, Mukund and Karypis, George},
  year = {2004},
  month = jan,
  journal = {ACM Trans. Inf. Syst.},
  volume = {22},
  number = {1},
  pages = {143--177},
  issn = {1046-8188},
  doi = {10.1145/963770.963776},
  urldate = {2015-12-02},
  abstract = {The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of items that will be of interest to a certain user. User-based collaborative filtering is the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers, which in typical commercial applications can be several millions. To address these scalability concerns model-based recommendation techniques have been developed. These techniques analyze the user--item matrix to discover relations between the different items and use these relations to compute the list of recommendations.In this article, we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality.},
  keywords = {CAREER,e-commerce predicting user behavior world wide web,Fair Info Access Paper,LensKit References}
}

@techreport{ekstrand:notation,
  title = {Recommender {{Systems Notation}}},
  author = {Ekstrand, Michael D and Konstan, Joseph A},
  year = {2019},
  number = {177},
  institution = {Boise State University},
  doi = {10.18122/cs_facpubs/177/boisestate},
  urldate = {2020-05-11},
  abstract = {As the field of recommender systems has developed, authors have used a myriad of notations for describing the mathematical workings of recommendation algorithms. These notations appear in research papers, books, lecture notes, blog posts, and software documentation. The disciplinary diversity of the field has not contributed to consistency in notation; scholars whose home base is in information retrieval have different habits and expectations than those in machine learning or human-computer interaction. In the course of years of teaching and research on recommender systems, we have seen the value in adopting a consistent notation across our work. This has been particularly highlighted in our development of the Recommender Systems MOOC on Coursera (Konstan et al. 2015), as we need to explain a wide variety of algorithms and our learners are not well-served by changing notation between algorithms. In this paper, we describe the notation we have adopted in our work, along with its justification and some discussion of considered alternatives. We present this in hope that it will be useful to others writing and teaching about recommender systems. This notation has served us well for some time now, in research, online education, and traditional classroom instruction. We feel it is ready for broad use.},
  keywords = {LensKit References,REU2020/Exposure}
}

@inproceedings{ekstrandAllCoolKids2018,
  title = {All {{The Cool Kids}}, {{How Do They Fit In}}?: {{Popularity}} and {{Demographic Biases}} in {{Recommender Evaluation}} and {{Effectiveness}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Ekstrand, Michael D and Tian, Mucun and Azpiazu, Ion Madrazo and Ekstrand, Jennifer D and Anuyah, Oghenemaro and McNeill, David and Pera and Soledad, Maria},
  year = {2018},
  month = feb,
  series = {{{PMLR}}},
  volume = {81},
  pages = {172--186},
  url = {http://proceedings.mlr.press/v81/ekstrand18b.html},
  keywords = {My Papers,Privacy and Fairness,Research Using LensKit}
}

@article{ekstrandDependencyInjectionStatic2016,
  title = {Dependency {{Injection}} with {{Static Analysis}} and {{Context-Aware Policy}}.},
  author = {Ekstrand, Michael D. and Ludwig, Michael},
  year = {2016},
  month = feb,
  journal = {Journal of Object Technology},
  volume = {15},
  number = {1},
  pages = {1:1},
  issn = {1660-1769},
  doi = {10.5381/jot.2016.15.5.a1},
  urldate = {2016-03-11},
  langid = {english}
}

@inproceedings{ekstrandExploringAuthorGender2018,
  title = {Exploring {{Author Gender}} in {{Book Rating}} and {{Recommendation}}},
  booktitle = {Proceedings of the {{Twelfth ACM Conference}} on {{Recommender Systems}}},
  author = {Ekstrand, Michael D and Tian, Mucun and Imran Kazi, Mohammed R and Mehrpouyan, Hoda and Kluver, Daniel},
  year = {2018},
  publisher = {ACM},
  keywords = {My Papers,Research Using LensKit}
}

@inproceedings{ekstrandSturgeonCoolKids2017,
  title = {Sturgeon and the {{Cool Kids}}: {{Problems}} with {{Top-N Recommender Evaluation}}},
  booktitle = {Proceedings of the 30th {{Florida Artificial Intelligence Research Society Conference}}},
  author = {Ekstrand, Michael D and Mahant, Vaibhav},
  year = {2017},
  month = may,
  publisher = {AAAI Press},
  url = {https://aaai.org/ocs/index.php/FLAIRS/FLAIRS17/paper/viewPaper/15534},
  abstract = {Top-N evaluation of recommender systems, typically carried out using metrics from information retrieval or machine learning, has several challenges. Two of these challenges are popularity bias, where the evaluation intrinsically favors algorithms that recommend popular items, and misclassified decoys, where items for which no user relevance is known are actually relevant to the user, but the evaluation is unaware and penalizes the recommender for suggesting them. One strategy for mitigating the misclassified decoy problem is the one-plus-random evaluation strategy and its generalization, which we call random decoys. In this work, we explore the random decoy strategy through both a theoretical treatment and an empirical study, but find little evidence to guide its tuning and show that it has complex and deleterious interactions with popularity bias.},
  keywords = {CAREER,Dagstuhl Perspectives IR Eval for RecSys,My Papers,Research Using LensKit,Zotero Import (Mar 30),Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/My Papers}
}

@inproceedings{ekstrandSturgeonCoolKids2017a,
  title = {Sturgeon and the {{Cool Kids}}: {{Problems}} with {{Top-N Recommender Evaluation}}},
  booktitle = {Proceedings of the 30th {{Florida Artificial Intelligence Research Society Conference}}},
  author = {Ekstrand, Michael D and Mahant, Vaibhav},
  year = {2017},
  month = may,
  series = {{{FLAIRS}} 30},
  publisher = {AAAI Press},
  url = {https://aaai.org/papers/639-flairs-2017-15534/},
  abstract = {Top-N evaluation of recommender systems, typically carried out using metrics from information retrieval or machine learning, has several challenges. Two of these challenges are popularity bias, where the evaluation intrinsically favors algorithms that recommend popular items, and misclassified decoys, where items for which no user relevance is known are actually relevant to the user, but the evaluation is unaware and penalizes the recommender for suggesting them. One strategy for mitigating the misclassified decoy problem is the one-plus-random evaluation strategy and its generalization, which we call random decoys. In this work, we explore the random decoy strategy through both a theoretical treatment and an empirical study, but find little evidence to guide its tuning and show that it has complex and deleterious interactions with popularity bias.}
}

@misc{ekstrandTestingRecommenders2016,
  title = {Testing {{Recommenders}}},
  author = {Ekstrand, Michael},
  year = {2016},
  month = feb,
  journal = {A Practical Guide to Building Recommender Systems},
  url = {https://buildingrecommenders.wordpress.com/2016/02/04/testing-recommenders/},
  urldate = {2017-01-06},
  abstract = {Why Test? When I met fellow GroupLens alum Sean McNee, he had a bit of advice for me: Write tests for your code. It took me some time to grasp the wisdom of this --- after all, isn't it just re{\dots}}
}

@inproceedings{ekstrandWhenRecommendersFail2012,
  title = {When {{Recommenders Fail}}: {{Predicting Recommender Failure}} for {{Algorithm Selection}} and {{Combination}}},
  shorttitle = {When Recommenders Fail},
  booktitle = {Proceedings of the {{Sixth ACM Conference}} on {{Recommender Systems}}},
  author = {Ekstrand, Michael and Riedl, John},
  year = {2012},
  series = {{{RecSys}} '12},
  pages = {233--236},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2365952.2366002},
  urldate = {2012-12-13},
  abstract = {Hybrid recommender systems --- systems using multiple algorithms together to improve recommendation quality --- have been well-known for many years and have shown good performance in recent demonstrations such as the NetFlix Prize. Modern hybridization techniques, such as feature-weighted linear stacking, take advantage of the hypothesis that the relative performance of recommenders varies by circumstance and attempt to optimize each item score to maximize the strengths of the component recommenders. Less attention, however, has been paid to understanding what these strengths and failure modes are. Understanding what causes particular recommenders to fail will facilitate better selection of the component recommenders for future hybrid systems and a better understanding of how individual recommender personalities can be harnessed to improve the recommender user experience. We present an analysis of the predictions made by several well-known recommender algorithms on the MovieLens 10M data set, showing that for many cases in which one algorithm fails, there is another that will correctly predict the rating.},
  isbn = {978-1-4503-1270-7},
  keywords = {CAREER,My Papers,Research Using LensKit,Zotero Import (Mar 30),Zotero Import (Mar 30)/Group Libraries/LensKit,Zotero Import (Mar 30)/Group Libraries/PiReT,Zotero Import (Mar 30)/Group Libraries/PiReT/LensKit,Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/My Papers,Zotero Import (Mar 30)/My Library/Thesis}
}

@article{friedmanRegularizationPathsGeneralized2010,
  title = {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author = {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
  year = {2010},
  month = feb,
  journal = {Journal of Statistical Software},
  volume = {33},
  number = {1},
  pages = {1--22},
  issn = {1548-7660},
  doi = {10.18637/jss.v033.i01},
  urldate = {2019-07-17},
  copyright = {Copyright (c) 2009 Jerome H. Friedman, Trevor Hastie, Rob Tibshirani},
  langid = {english}
}

@misc{funkNetflixUpdateTry2006,
  title = {Netflix {{Update}}: {{Try This}} at {{Home}}},
  author = {Funk, Simon},
  year = {2006},
  month = dec,
  url = {http://sifter.org/~simon/journal/20061211.html},
  urldate = {2010-04-08},
  keywords = {Zotero Import (Mar 30),Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/Eval Grant,Zotero Import (Mar 30)/My Library/LensKit,Zotero Import (Mar 30)/My Library/Recommender Systems,Zotero Import (Mar 30)/My Library/Recommender Systems/Error Analysis Paper,Zotero Import (Mar 30)/My Library/Recommender Systems/List Comparison Paper,Zotero Import (Mar 30)/My Library/Thesis}
}

@inproceedings{gantnerMyMediaLiteFreeRecommender2011,
  title = {{{MyMediaLite}}: {{A Free Recommender System Library}}},
  booktitle = {Proceedings of the {{Fifth ACM Conference}} on {{Recommender Systems}}},
  author = {Gantner, Zeno and Rendle, Steffen and Freudenthaler, Christoph and {Schmidt-Thieme}, Lars},
  year = {2011},
  series = {{{RecSys}} '11},
  pages = {305--308},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2043932.2043989},
  abstract = {MyMediaLite is a fast and scalable, multi-purpose library of recommender system algorithms, aimed both at recommender system researchers and practitioners. It addresses two common scenarios in collaborative filtering: rating prediction (e.g. on a scale of 1 to 5 stars) and item prediction from positive-only implicit feedback (e.g. from clicks or purchase actions). The library offers state-of-the-art algorithms for those two tasks. Programs that expose most of the library's functionality, plus a GUI demo, are included in the package. Efficient data structures and a common API are used by the implemented algorithms, and may be used to implement further algorithms. The API also contains methods for real-time updates and loading/storing of already trained recommender models. MyMediaLite is free/open source software, distributed under the terms of the GNU General Public License (GPL). Its methods have been used in four different industrial field trials of the MyMedia project, including one trial involving over 50,000 households.},
  isbn = {978-1-4503-0683-6}
}

@article{gopalanScalableRecommendationPoisson2013a,
  title = {Scalable {{Recommendation}} with {{Poisson Factorization}}},
  author = {Gopalan, Prem and Hofman, Jake M and Blei, David M},
  year = {2013},
  month = nov,
  journal = {arXiv:1311.1704 [cs, stat]},
  eprint = {1311.1704},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1311.1704},
  urldate = {2017-02-09},
  abstract = {We develop a Bayesian Poisson matrix factorization model for forming recommendations from sparse user behavior data. These data are large user/item matrices where each user has provided feedback on only a small subset of items, either explicitly (e.g., through star ratings) or implicitly (e.g., through views or purchases). In contrast to traditional matrix factorization approaches, Poisson factorization implicitly models each user's limited attention to consume items. Moreover, because of the mathematical form of the Poisson likelihood, the model needs only to explicitly consider the observed entries in the matrix, leading to both scalable computation and good predictive performance. We develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets. This is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations. We apply our method to large real-world user data containing users rating movies, users listening to songs, and users reading scientific papers. In all these settings, Bayesian Poisson factorization outperforms state-of-the-art matrix factorization methods.},
  archiveprefix = {arXiv},
  keywords = {LensKit References,Zotero Import (Mar 30),Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/Recommender Systems/Matrix Factorization}
}

@inproceedings{groverStochasticOptimizationSorting2019,
  title = {Stochastic {{Optimization}} of {{Sorting Networks}} via {{Continuous Relaxations}}},
  booktitle = {Proceedings of the {{Seventh International Conference}} on {{Learning Representations}}},
  author = {Grover, Aditya and Wang, Eric and Zweig, Aaron and Ermon, Stefano},
  year = {2019},
  month = mar,
  url = {https://openreview.net/forum?id=H1eSS3CcKX},
  abstract = {Sorting input objects is an important step in many machine learning pipelines. However, the sorting operator is non-differentiable with respect to its inputs, which prohibits end-to-end gradient-based optimization. In this work, we propose NeuralSort, a general-purpose continuous relaxation of the output of the sorting operator from permutation matrices to the set of unimodal row-stochastic matrices, where every row sums to one and has a distinct arg max. This relaxation permits straight-through optimization of any computational graph involve a sorting operation. Further, we use this relaxation to enable gradient-based stochastic optimization over the combinatorially large space of permutations by deriving a reparameterized gradient estimator for the Plackett-Luce family of distributions over permutations. We demonstrate the usefulness of our framework on three tasks that require learning semantic orderings of high-dimensional objects, including a fully differentiable, parameterized extension of the k-nearest neighbors algorithm.},
  keywords = {LensKit References}
}

@article{herlockerEmpiricalAnalysisDesign2002,
  title = {An {{Empirical Analysis}} of {{Design Choices}} in {{Neighborhood-Based Collaborative Filtering Algorithms}}},
  author = {Herlocker, Jon and Konstan, Joseph A. and Riedl, John},
  year = {2002},
  month = oct,
  journal = {Information Retrieval},
  volume = {5},
  number = {4},
  pages = {287--310},
  issn = {1386-4564, 1573-7659},
  doi = {10.1023/A:1020443909834},
  urldate = {2015-12-02},
  abstract = {Collaborative filtering systems predict a user's interest in new items based on the recommendations of other people with similar interests. Instead of performing content indexing or content analysis, collaborative filtering systems rely entirely on interest ratings from members of a participating community. Since predictions are based on human ratings, collaborative filtering systems have the potential to provide filtering based on complex attributes, such as quality, taste, or aesthetics. Many implementations of collaborative filtering apply some variation of the neighborhood-based prediction algorithm. Many variations of similarity metrics, weighting approaches, combination measures, and rating normalization have appeared in each implementation. For these parameters and others, there is no consensus as to which choice of technique is most appropriate for what situations, nor how significant an effect on accuracy each parameter has. Consequently, every person implementing a collaborative filtering system must make hard design choices with little guidance. This article provides a set of recommendations to guide design of neighborhood-based prediction systems, based on the results of an empirical study. We apply an analysis framework that divides the neighborhood-based prediction approach into three components and then examines variants of the key parameters in each component. The three components identified are similarity computation, neighbor selection, and rating combination.},
  langid = {english},
  keywords = {CAREER,Fair Info Access Paper,Fall 2017 IR Fairness,LensKit References}
}

@inproceedings{hu:implicit-mf,
  title = {Collaborative {{Filtering}} for {{Implicit Feedback Datasets}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Hu, Y and Koren, Y and Volinsky, C},
  year = {2008},
  month = dec,
  pages = {263--272},
  publisher = {ieeexplore.ieee.org},
  doi = {10.1109/ICDM.2008.22},
  abstract = {A common task of recommender systems is to improve customer experience through personalized recommendations based on prior implicit feedback. These systems passively track different sorts of user behavior, such as purchase history, watching habits and browsing activity, in order to model user preferences. Unlike the much more extensively researched explicit feedback, we do not have any direct input from the users regarding their preferences. In particular, we lack substantial evidence on which products consumer dislike. In this work we identify unique properties of implicit feedback datasets. We propose treating the data as indication of positive and negative preference associated with vastly varying confidence levels. This leads to a factor model which is especially tailored for implicit feedback recommenders. We also suggest a scalable optimization procedure, which scales linearly with the data size. The algorithm is used successfully within a recommender system for television shows. It compares favorably with well tuned implementations of other known methods. In addition, we offer a novel way to give explanations to recommendations given by this factor model.},
  keywords = {browsing activity,collaborative filtering,Collaborative filtering,customer experience,Data mining,Demography,electronic commerce,feedback,Filtering,History,implicit feedback,implicit feedback datasets,International collaboration,LensKit References,Motion pictures,Negative feedback,personalized recommendations,purchase history,recommender system,recommender systems,Recommender systems,scalable optimization procedure,TV,user preferences,Watches,watching habits}
}

@book{hugSurprisePythonLibrary2017,
  title = {Surprise, a {{Python Library}} for {{Recommender Systems}}},
  author = {Hug, Nicolas},
  year = {2017},
  url = {http://surpriselib.com}
}

@inproceedings{jeunenNormalisedDiscountedCumulative2024,
  title = {On ({{Normalised}}) {{Discounted Cumulative Gain}} as an {{Off-Policy Evaluation Metric}} for {{Top-n Recommendation}}},
  booktitle = {Proceedings of the 30th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Jeunen, Olivier and Potapov, Ivan and Ustimenko, Aleksei},
  year = {2024},
  month = aug,
  series = {{{KDD}} '24},
  pages = {1222--1233},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3637528.3671687},
  urldate = {2025-01-13},
  abstract = {Approaches to recommendation are typically evaluated in one of two ways: (1) via a (simulated) online experiment, often seen as the gold standard, or (2) via some offline evaluation procedure, where the goal is to approximate the outcome of an online experiment. Several offline evaluation metrics have been adopted in the literature, inspired by ranking metrics prevalent in the field of Information Retrieval. (Normalised) Discounted Cumulative Gain (nDCG) is one such metric that has seen widespread adoption in empirical studies, and higher (n)DCG values have been used to present new methods as the state-of-the-art in top-n recommendation for many years.Our work takes a critical look at this approach, and investigates when we can expect such metrics to approximate the gold standard outcome of an online experiment. We formally present the assumptions that are necessary to consider DCG an unbiased estimator of online reward and provide a derivation for this metric from first principles, highlighting where we deviate from its traditional uses in IR. Importantly, we show that normalising the metric renders it inconsistent, in that even when DCG is unbiased, ranking competing methods by their normalised DCG can invert their relative order. Through a correlation analysis between off- and on-line experiments conducted on a large-scale recommendation platform, we show that our unbiased DCG estimates strongly correlate with online reward, even when some of the metric's inherent assumptions are violated. This statement no longer holds for its normalised variant, suggesting that nDCG's practical utility may be limited.},
  isbn = {9798400704901}
}

@book{kluverBookLens2014,
  title = {{{BookLens}}},
  author = {Kluver, Daniel and Ludwig, Michael and Davies, Richard T. and Konstan, Joseph A. and Riedl, John T.},
  year = {2014},
  publisher = {GroupLens Research, University of Minnesota},
  url = {https://booklens.umn.edu/}
}

@inproceedings{kluverEvaluatingRecommenderBehavior2014,
  title = {Evaluating {{Recommender Behavior}} for {{New Users}}},
  booktitle = {Proceedings of the {{Eighth ACM Conference}} on {{Recommender Systems}} ({{RecSys}} '14)},
  author = {Kluver, Daniel and Konstan, Joseph A.},
  year = {2014},
  month = oct,
  publisher = {ACM},
  doi = {10.1145/2645710.2645742},
  keywords = {CAREER,Research Using LensKit,Zotero Import (Mar 30),Zotero Import (Mar 30)/Group Libraries/LensKit}
}

@inproceedings{kluverHowManyBits2012,
  title = {How {{Many Bits}} per {{Rating}}?},
  booktitle = {Proceedings of the Sixth {{ACM}} Conference on {{Recommender}} Systems},
  author = {Kluver, Daniel and Nguyen, Tien T. and Ekstrand, Michael and Sen, Shilad and Riedl, John},
  year = {2012},
  series = {{{RecSys}} '12},
  pages = {99--106},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2365952.2365974},
  urldate = {2013-09-12},
  abstract = {Most recommender systems assume user ratings accurately represent user preferences. However, prior research shows that user ratings are imperfect and noisy. Moreover, this noise limits the measurable predictive power of any recommender system. We propose an information theoretic framework for quantifying the preference information contained in ratings and predictions. We computationally explore the properties of our model and apply our framework to estimate the efficiency of different rating scales for real world datasets. We then estimate how the amount of information predictions give to users is related to the scale ratings are collected on. Our findings suggest a tradeoff in rating scale granularity: while previous research indicates that coarse scales (such as thumbs up / thumbs down) take less time, we find that ratings with these scales provide less predictive value to users. We introduce a new measure, preference bits per second, to quantitatively reconcile this tradeoff.},
  isbn = {978-1-4503-1270-7},
  keywords = {CAREER,My Papers,Research Using LensKit,Zotero Import (Mar 30),Zotero Import (Mar 30)/Group Libraries/LensKit,Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/My Papers,Zotero Import (Mar 30)/My Library/Recommender Systems/Class Spring 2017,Zotero Import (Mar 30)/My Library/Thesis}
}

@article{konstanTeachingRecommenderSystems2015,
  title = {Teaching {{Recommender Systems}} at {{Large Scale}}: {{Evaluation}} and {{Lessons Learned}} from a {{Hybrid MOOC}}},
  author = {Konstan, Joseph A and Walker, J D and Brooks, D Christopher and Brown, Keith and Ekstrand, Michael D},
  year = {2015},
  month = apr,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {22},
  number = {2},
  pages = {10:1--10:23},
  issn = {1073-0516},
  doi = {10.1145/2728171},
  abstract = {In the fall of 2013, we offered an open online Introduction to Recommender Systems through Coursera, while simultaneously offering a for-credit version of the course on-campus using the Coursera platform and a flipped classroom instruction model. As the goal of offering this course was to experiment with this type of instruction, we performed extensive evaluation including surveys of demographics, self-assessed skills, and learning intent; we also designed a knowledge-assessment tool specifically for the subject matter in this course, administering it before and after the course to measure learning, and again 5 months later to measure retention. We also tracked students through the course, including separating out students enrolled for credit from those enrolled only for the free, open course. Students had significant knowledge gains across all levels of prior knowledge and across all demographic categories. The main predictor of knowledge gain was effort expended in the course. Students also had significant knowledge retention after the course. Both of these results are limited to the sample of students who chose to complete our knowledge tests. Student completion of the course was hard to predict, with few factors contributing predictive power; the main predictor of completion was intent to complete. Students who chose a concepts-only track with hand exercises achieved the same level of knowledge of recommender systems concepts as those who chose a programming track and its added assignments, though the programming students gained additional programming knowledge. Based on the limited data we were able to gather, face-to-face students performed as well as the online-only students or better; they preferred this format to traditional lecture for reasons ranging from pure convenience to the desire to watch videos at a different pace (slower for English language learners; faster for some native English speakers). This article also includes our qualitative observations, lessons learned, and future directions.}
}

@article{korenMatrixFactorizationTechniques2009,
  title = {Matrix {{Factorization Techniques}} for {{Recommender Systems}}},
  author = {Koren, Y and Bell, R and Volinsky, C},
  year = {2009},
  month = aug,
  journal = {Computer},
  volume = {42},
  number = {8},
  pages = {30--37},
  issn = {0018-9162},
  doi = {10.1109/MC.2009.263},
  abstract = {As the Netflix Prize competition has demonstrated, matrix factorization models are superior to classic nearest neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.},
  keywords = {CAREER,Fair Info Access Paper,LensKit References}
}

@inproceedings{lamNumbaLLVMbasedPython2015,
  title = {Numba: {{A LLVM-based Python JIT Compiler}}},
  shorttitle = {Numba},
  booktitle = {Proceedings of the {{Second Workshop}} on the {{LLVM Compiler Infrastructure}} in {{HPC}}},
  author = {Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  year = {2015},
  series = {{{LLVM}} '15},
  pages = {7:1--7:6},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2833157.2833162},
  urldate = {2019-05-30},
  abstract = {Dynamic, interpreted languages, like Python, are attractive for domain-experts and scientists experimenting with new ideas. However, the performance of the interpreter is often a barrier when scaling to larger data sets. This paper presents a just-in-time compiler for Python that focuses in scientific and array-oriented computing. Starting with the simple syntax of Python, Numba compiles a subset of the language into efficient machine code that is comparable in performance to a traditional compiled language. In addition, we share our experience in building a JIT compiler using LLVM[1].},
  isbn = {978-1-4503-4005-2}
}

@inproceedings{lenskit-java,
  title = {Rethinking the {{Recommender Research Ecosystem}}: {{Reproducibility}}, {{Openness}}, and {{LensKit}}},
  shorttitle = {Rethinking the {{Recommender Research Ecosystem}}},
  booktitle = {Proceedings of the 5th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Ekstrand, Michael and Ludwig, Michael and Konstan, Joseph A. and Riedl, John},
  year = {2011},
  pages = {133--140},
  publisher = {ACM},
  doi = {10.1145/2043932.2043958},
  urldate = {2012-04-07},
  abstract = {Recommender systems research is being slowed by the difficulty of replicating and comparing research results. Published research uses various experimental methodologies and metrics that are difficult to compare. It also often fails to sufficiently document the details of proposed algorithms or the evaluations employed. Researchers waste time reimplementing well-known algorithms, and the new implementations may miss key details from the original algorithm or its subsequent refinements. When proposing new algorithms, researchers should compare them against finely-tuned implementations of the leading prior algorithms using state-of-the-art evaluation methodologies. With few exceptions, published algorithmic improvements in our field should be accompanied by working code in a standard framework, including test harnesses to reproduce the described results. To that end, we present the design and freely distributable source code of LensKit, a flexible platform for reproducible recommender systems research. LensKit provides carefully tuned implementations of the leading collaborative filtering algorithms, APIs for common recommender system use cases, and an evaluation framework for performing reproducible offline evaluations of algorithms. We demonstrate the utility of LensKit by replicating and extending a set of prior comparative studies of recommender algorithms --- showing limitations in some of the original results --- and by investigating a question recently raised by a leader in the recommender systems community on problems with error-based prediction evaluation.},
  isbn = {978-1-4503-0683-6},
  keywords = {CAREER,LensKit References}
}

@inproceedings{lkpy,
  title = {{{LensKit}} for {{Python}}: {{Next-Generation Software}} for {{Recommender System Experiments}}},
  booktitle = {Proceedings of the 29th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Ekstrand, Michael D},
  year = {2020},
  doi = {10.1145/3340531.3412778},
  keywords = {LensKit References,PIReT Papers}
}

@misc{martinabadiTensorFlowLargeScaleMachine2015,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {{Mart{\'i}n Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Man{\'e}} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Vi{\'e}gas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
  year = {2015},
  url = {https://www.tensorflow.org/},
  keywords = {LensKit References}
}

@inproceedings{mckinneyDataStructuresStatistical2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {McKinney, Wes and {Others}},
  year = {2010},
  volume = {445},
  pages = {51--56},
  publisher = {Austin, TX},
  url = {http://conference.scipy.org/proceedings/scipy2010/pdfs/mckinney.pdf},
  keywords = {LensKit References,Software}
}

@book{mckinneyPythonDataAnalysis2018,
  title = {Python for {{Data Analysis}}: {{Data Wrangling}} with Pandas, {{NumPy}}, and {{IPython}}},
  author = {McKinney, Wes},
  year = {2018},
  publisher = {O'Reilly},
  url = {http://shop.oreilly.com/product/0636920023784.do},
  isbn = {978-1-4919-5766-0},
  keywords = {LensKit References,Software}
}

@article{movielens,
  title = {The {{MovieLens Datasets}}: {{History}} and {{Context}}},
  shorttitle = {The {{MovieLens Datasets}}},
  author = {Harper, F. Maxwell and Konstan, Joseph A.},
  year = {2015},
  month = dec,
  journal = {ACM Transactions on Interactive Intelligent Systems},
  volume = {5},
  number = {4},
  pages = {19:1--19:19},
  issn = {2160-6455},
  doi = {10.1145/2827872},
  urldate = {2016-03-11},
  abstract = {The MovieLens datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the MovieLens movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of MovieLens and the MovieLens datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the MovieLens datasets in new research.},
  keywords = {CAREER,dataset,Fair Info Access Paper,KidRec,LensKit References,Research Using LensKit,Zotero Import (Mar 30),Zotero Import (Mar 30)/Group Libraries/LensKit,Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/Data Sets,Zotero Import (Mar 30)/My Library/Recommender Systems}
}

@article{ndcg,
  title = {Cumulated Gain-Based Evaluation of {{IR}} Techniques},
  author = {J{\"a}rvelin, Kalervo and Kek{\"a}l{\"a}inen, Jaana},
  year = {2002},
  month = oct,
  journal = {ACM Transactions on Information Systems},
  volume = {20},
  number = {4},
  pages = {422--446},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1094-9224},
  doi = {10.1145/582415.582418},
  keywords = {CAREER,Graded relevance judgments cumulated gain,LensKit References,ndcg}
}

@inproceedings{ningSLIMSparseLinear2011,
  title = {{{SLIM}}: {{Sparse Linear Methods}} for {{Top-N Recommender Systems}}},
  booktitle = {{{ICDM}} '11},
  author = {Ning, Xia and Karypis, George},
  year = {2011},
  pages = {497--506},
  publisher = {IEEE Computer Society},
  address = {Washington, DC, USA},
  doi = {10.1109/ICDM.2011.134},
  urldate = {2017-01-04},
  abstract = {This paper focuses on developing effective and efficient algorithms for top-N recommender systems. A novel Sparse Linear Method (SLIM) is proposed, which generates top-N recommendations by aggregating from user purchase/rating profiles. A sparse aggregation coefficient matrix W is learned from SLIM by solving an `1-norm and `2-norm regularized optimization problem. W is demonstrated to produce high quality recommendations and its sparsity allows SLIM to generate recommendations very fast. A comprehensive set of experiments is conducted by comparing the SLIM method and other state-of-the-art top-N recommendation methods. The experiments show that SLIM achieves significant improvements both in run time performance and recommendation quality over the best existing methods.},
  keywords = {CAREER,Fair Info Access Paper,LensKit References,Zotero Import (Mar 30),Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/Recommender Systems}
}

@book{oliphantGuideNumPy2006,
  title = {A {{Guide}} to {{NumPy}}},
  author = {Oliphant, Travis E},
  year = {2006},
  publisher = {Trelgol Publishing},
  keywords = {LensKit References,Software}
}

@article{oliphantPythonScientificComputing2007,
  title = {Python for {{Scientific Computing}}},
  author = {Oliphant, T E},
  year = {2007},
  month = may,
  journal = {Computing in Science Engineering},
  volume = {9},
  number = {3},
  pages = {10--20},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2007.58},
  abstract = {Python is an excellent ``steering'' language for scientific codes written in other languages. However, with additional basic tools, Python transforms into a high-level language suited for scientific and engineering code that's often fast enough to be immediately useful but also flexible enough to be sped up with additional extensions.},
  keywords = {Application software,computer languages,Embedded software,high level languages,High level languages,high-level language,Internet,LensKit References,Libraries,Prototypes,Python,scientific codes,scientific computing,Scientific computing,scientific programming,Software,Software standards,Standards development,steering language,Writing}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F and Varoquaux, G and Gramfort, A and Michel, V and Thirion, B and Grisel, O and Blondel, M and Prettenhofer, P and Weiss, R and Dubourg, V and Vanderplas, J and Passos, A and Cournapeau, D and Brucher, M and Perrot, M and Duchesnay, E},
  year = {2011},
  journal = {J. Mach. Learn. Res.},
  volume = {12},
  pages = {2825--2830},
  issn = {1532-4435},
  keywords = {LensKit References}
}

@article{peraRecommendingBooksBe2017,
  title = {Recommending Books to Be Exchanged Online in the Absence of Wish Lists},
  author = {Pera, Maria Soledad and Ng, Yiu-Kai},
  year = {2017},
  month = nov,
  journal = {Journal of the Association for Information Science and Technology},
  issn = {2330-1643},
  doi = {10.1002/asi.23978},
  abstract = {An online exchange system is a web service that allows communities to trade items without the burden of manually selecting them, which saves users' time and effort. Even though online book-exchange systems have been developed, their services can further be improved by reducing the workload imposed on their users. To accomplish this task, we propose a recommendation-based book exchange system, called EasyEx, which identifies potential exchanges for a user solely based on a list of items the user is willing to part with. EasyEx is a novel and unique book-exchange system because unlike existing online exchange systems, it does not require a user to create and maintain a wish list, which is a list of items the user would like to receive as part of the exchange. Instead, EasyEx directly suggests items to users to increase serendipity and as a result expose them to items which may be unfamiliar, but appealing, to them. In identifying books to be exchanged, EasyEx employs known recommendation strategies, that is, personalized mean and matrix factorization, to predict book ratings, which are treated as the degrees of appeal to a user on recommended books. Furthermore, EasyEx incorporates OptaPlanner, which solves constraint satisfaction problems efficiently, as part of the recommendation-based exchange process to create exchange cycles. Experimental results have verified that EasyEx offers users recommended books that satisfy the users' interests and contributes to the item-exchange mechanism with a new design methodology.},
  keywords = {Research Using LensKit}
}

@article{pessemierHybridGroupRecommendations2016,
  title = {Hybrid {{Group Recommendations}} for a {{Travel Service}}},
  author = {Pessemier, Toon De and Dhondt, Jeroen and Martens, Luc},
  year = {2016},
  month = jan,
  journal = {Multimedia Tools and Applications},
  volume = {75},
  number = {5},
  pages = {1--25},
  issn = {1380-7501, 1573-7721},
  doi = {10.1007/s11042-016-3265-x},
  urldate = {2016-03-11},
  abstract = {Recommendation techniques have proven their usefulness as a tool to cope with the information overload problem in many classical domains such as movies, books, and music. Additional challenges for recommender systems emerge in the domain of tourism such as acquiring metadata and feedback, the sparsity of the rating matrix, user constraints, and the fact that traveling is often a group activity. This paper proposes a recommender system that offers personalized recommendations for travel destinations to individuals and groups. These recommendations are based on the users' rating profile, personal interests, and specific demands for their next destination. The recommendation algorithm is a hybrid approach combining a content-based, collaborative filtering, and knowledge-based solution. For groups of users, such as families or friends, individual recommendations are aggregated into group recommendations, with an additional opportunity for users to give feedback on these group recommendations. A group of test users evaluated the recommender system using a prototype web application. The results prove the usefulness of individual and group recommendations and show that users prefer the hybrid algorithm over each individual technique. This paper demonstrates the added value of various recommendation algorithms in terms of different quality aspects, compared to an unpersonalized list of the most-popular destinations.},
  langid = {english},
  keywords = {CAREER,Research Using LensKit,Zotero Import (Mar 30),Zotero Import (Mar 30)/Group Libraries/LensKit}
}

@inproceedings{pilaszyFastALSbasedMatrix2010,
  title = {Fast {{ALS-based Matrix Factorization}} for {{Explicit}} and {{Implicit Feedback Datasets}}},
  booktitle = {{{RecSys}} '10},
  author = {Pil{\'a}szy, Istv{\'a}n and Zibriczky, D{\'a}vid and Tikk, Domonkos},
  year = {2010},
  pages = {71--78},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1864708.1864726},
  urldate = {2015-06-01},
  abstract = {Alternating least squares (ALS) is a powerful matrix factorization (MF) algorithm for both explicit and implicit feedback based recommender systems. As shown in many articles, increasing the number of latent factors (denoted by K) boosts the prediction accuracy of MF based recommender systems, including ALS as well. The price of the better accuracy is paid by the increased running time: the running time of the original version of ALS is proportional to K3. Yet, the running time of model building can be important in recommendation systems; if the model cannot keep up with the changing item portfolio and/or user profile, the prediction accuracy can be degraded. In this paper we present novel and fast ALS variants both for the implicit and explicit feedback datasets, which offers better trade-off between running time and accuracy. Due to the significantly lower computational complexity of the algorithm - linear in terms of K - the model being generated under the same amount of time is more accurate, since the faster training enables to build model with more latent factors. We demonstrate the efficiency of our ALS variants on two datasets using two performance measures, RMSE and average relative position (ARP), and show that either a significantly more accurate model can be generated under the same amount of time or a model with similar prediction accuracy can be created faster; for explicit feedback the speed-up factor can be even 5-10.},
  keywords = {CAREER,LensKit References}
}

@inproceedings{pilaszyRecommendingNewMovies2009,
  title = {Recommending {{New Movies}}: {{Even}} a {{Few Ratings Are More Valuable Than Metadata}}},
  booktitle = {{{RecSys}} '09},
  author = {Pil{\'a}szy, Istv{\'a}n and Tikk, Domonkos},
  year = {2009},
  pages = {93--100},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/1639714.1639731},
  urldate = {2016-12-14},
  abstract = {The Netflix Prize (NP) competition gave much attention to collaborative filtering (CF) approaches. Matrix factorization (MF) based CF approaches assign low dimensional feature vectors to users and items. We link CF and content-based filtering (CBF) by finding a linear transformation that transforms user or item descriptions so that they are as close as possible to the feature vectors generated by MF for CF. We propose methods for explicit feedback that are able to handle 140,000 features when feature vectors are very sparse. With movie metadata collected for the NP movies we show that the prediction performance of the methods is comparable to that of CF, and can be used to predict user preferences on new movies. We also investigate the value of movie metadata compared to movie ratings in regards of predictive power. We compare our solely CBF approach with a simple baseline rating-based predictor. We show that even 10 ratings of a new movie are more valuable than its metadata for predicting user ratings.},
  keywords = {CAREER,LensKit References}
}

@article{rbp,
  title = {Rank-{{Biased Precision}} for {{Measurement}} of {{Retrieval Effectiveness}}},
  author = {Moffat, Alistair and Zobel, Justin},
  year = {2008},
  month = dec,
  journal = {Transactions on Information Systems},
  volume = {27},
  number = {1},
  pages = {2:1-27},
  publisher = {ACM},
  issn = {1094-9224},
  doi = {10.1145/1416950.1416952},
  abstract = {A range of methods for measuring the effectiveness of information retrieval systems has been proposed. These are typically intended to provide a quantitative single-value summary of a document ranking relative to a query. However, many of these measures have failings {\dots}}
}

@inproceedings{rendleBPRBayesianPersonalized2009,
  title = {{{BPR}}: {{Bayesian Personalized Ranking}} from {{Implicit Feedback}}},
  booktitle = {{{UAI}} '09},
  author = {Rendle, Steffen and Freudenthaler, Christoph and Gantner, Zeno and {Schmidt-Thieme}, Lars},
  year = {2009},
  pages = {452--461},
  publisher = {AUAI Press},
  address = {Arlington, Virginia, United States},
  url = {http://dl.acm.org/citation.cfm?id=1795114.1795167},
  urldate = {2015-10-16},
  abstract = {Item recommendation is the task of predicting a personalized ranking on a set of items (e.g. websites, movies, products). In this paper, we investigate the most common scenario with implicit feedback (e.g. clicks, purchases). There are many methods for item recommendation from implicit feedback like matrix factorization (MF) or adaptive k-nearest-neighbor (kNN). Even though these methods are designed for the item prediction task of personalized ranking, none of them is directly optimized for ranking. In this paper we present a generic optimization criterion BPR-Opt for personalized ranking that is the maximum posterior estimator derived from a Bayesian analysis of the problem. We also provide a generic learning algorithm for optimizing models with respect to BPR-Opt. The learning method is based on stochastic gradient descent with bootstrap sampling. We show how to apply our method to two state-of-the-art recommender models: matrix factorization and adaptive kNN. Our experiments indicate that for the task of personalized ranking our optimization method outperforms the standard learning techniques for MF and kNN. The results show the importance of optimizing models for the right criterion.},
  keywords = {CAREER,Zotero Import (Mar 30),Zotero Import (Mar 30)/My Library,Zotero Import (Mar 30)/My Library/Recommender Systems,Zotero Import (Mar 30)/My Library/Recommender Systems/Class Spring 2017}
}

@inproceedings{resnickGroupLensOpenArchitecture1994,
  title = {{{GroupLens}}: {{An Open Architecture}} for {{Collaborative Filtering}} of {{Netnews}}},
  shorttitle = {{{GroupLens}}},
  booktitle = {Proceedings of the 1994 {{ACM Conference}} on {{Computer Supported Cooperative Work}}},
  author = {Resnick, Paul and Iacovou, Neophytos and Suchak, Mitesh and Bergstrom, Peter and Riedl, John},
  year = {1994},
  series = {{{CSCW}} '94},
  pages = {175--186},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/192844.192905},
  urldate = {2015-12-02},
  abstract = {Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed  independently and can interoperate with the components we have developed.},
  isbn = {978-0-89791-689-9},
  keywords = {CAREER,LensKit References,RecSys Foundation Papers}
}

@inproceedings{sarwarItembasedCollaborativeFiltering2001,
  title = {Item-Based {{Collaborative Filtering Recommendation Algorithms}}},
  booktitle = {Proceedings of the 10th {{International Conference}} on {{World Wide Web}}},
  author = {Sarwar, Badrul and Karypis, George and Konstan, Joseph and Riedl, John},
  year = {2001},
  series = {{{WWW}} '01},
  pages = {285--295},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/371920.372071},
  urldate = {2015-12-02},
  isbn = {978-1-58113-348-6},
  keywords = {CAREER,LensKit References,RecSys Foundation Papers}
}

@article{sepulvedaPyRecLabSoftwareLibrary2017,
  title = {{{pyRecLab}}: {{A Software Library}} for {{Quick Prototyping}} of {{Recommender Systems}}},
  author = {Sepulveda, Gabriel and Parra, Denis},
  year = {2017},
  journal = {arXiv preprint arXiv:1706. 06291},
  url = {https://arxiv.org/abs/1706.06291},
  abstract = {Abstract: This paper introduces pyRecLab , a software library written in C++ with Python bindings which allows to quickly train, test and develop recommender systems. Although there are several software libraries for this purpose, only a few let developers to get quickly}
}

@inproceedings{sklearn-api,
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  booktitle = {Workshop on {{Languages}} for {{Data Mining}} and {{Machine Learning}} at {{ECMLPKDD}} 2013},
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"e}l},
  year = {2013},
  month = sep,
  url = {http://arxiv.org/abs/1309.0238},
  abstract = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
  keywords = {LensKit References}
}

@mastersthesis{solvangVideoRecommendationSystems2017,
  title = {Video {{Recommendation Systems}}: {{Finding}} a {{Suitable Recommendation Approach}} for an {{Application Without Sufficient Data}}},
  author = {Solvang, Marius L{\o}rstad},
  year = {2017},
  url = {http://hdl.handle.net/10852/59239},
  keywords = {Research Using LensKit}
}

@inproceedings{takacsAlternatingLeastSquares2012,
  title = {Alternating {{Least Squares}} for {{Personalized Ranking}}},
  booktitle = {{{RecSys}} '12},
  author = {Tak{\'a}cs, G{\'a}bor and Tikk, Domonkos},
  year = {2012},
  month = sep,
  pages = {83--90},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2365952.2365972},
  abstract = {Two flavors of the recommendation problem are the explicit and the implicit feedback settings. In the explicit feedback case, users rate items and the user-item preference relationship can be modelled on the basis of the ratings. In the harder but more common implicit feedback case, the system has to infer user preferences from indirect information: presence or absence of events, such as a user viewed an item. One approach for handling implicit feedback is to minimize a ranking objective function instead of the conventional prediction mean squared error. The naive minimization of a ranking objective function is typically expensive. This difficulty is usually overcome by a trade-off: sacrificing the accuracy to some extent for computational efficiency by sampling the objective function. In this paper, we present a computationally effective approach for the direct minimization of a ranking objective function, without sampling. We demonstrate by experiments on the Y!Music and Netflix data sets that the proposed method outperforms other implicit feedback recommenders in many cases in terms of the ErrorRate, ARP and Recall evaluation metrics.},
  keywords = {collaborative filtering alternating least squares ranking,Exemplars/Introduction,LensKit References}
}

@inproceedings{takacsApplicationsConjugateGradient2011,
  title = {Applications of the {{Conjugate Gradient Method}} for {{Implicit Feedback Collaborative Filtering}}},
  booktitle = {Proceedings of the {{Fifth ACM Conference}} on {{Recommender Systems}}},
  author = {Tak{\'a}cs, G{\'a}bor and Pil{\'a}szy, Istv{\'a}n and Tikk, Domonkos},
  year = {2011},
  series = {{{RecSys}} '11},
  pages = {297--300},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2043932.2043987},
  urldate = {2019-07-12},
  abstract = {The need for solving weighted ridge regression (WRR) problems arises in a number of collaborative filtering (CF) algorithms. Often, there is not enough time to calculate the exact solution of the WRR problem, or it is not required. The conjugate gradient (CG) method is a state-of-the-art approach for the approximate solution of WRR problems. In this paper, we investigate some applications of the CG method for new and existing implicit feedback CF models. We demonstrate through experiments on the Netflix dataset that CG can be an efficient tool for training implicit feedback CF models.},
  isbn = {978-1-4503-0683-6},
  keywords = {collaborative filteringconjugate gradient method,LensKit References}
}

@inproceedings{tammQualityMetricsRecommender2021,
  title = {Quality {{Metrics}} in {{Recommender Systems}}: {{Do We Calculate Metrics Consistently}}?},
  booktitle = {{{RecSys}} '21},
  author = {Tamm, Yan-Martin and Damdinov, Rinchin and Vasilev, Alexey},
  year = {2021},
  month = sep,
  pages = {708--713},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3460231.3478848},
  urldate = {2021-10-04},
  abstract = {Offline evaluation is a popular approach to determine the best algorithm in terms of the chosen quality metric. However, if the chosen metric calculates something unexpected, this miscommunication can lead to poor decisions and wrong conclusions. In this paper, we thoroughly investigate quality metrics used for recommender systems evaluation. We look at the practical aspect of implementations found in modern RecSys libraries and at the theoretical aspect of definitions in academic papers. We find that Precision is the only metric universally understood among papers and libraries, while other metrics may have different interpretations. Metrics implemented in different libraries sometimes have the same name but measure different things, which leads to different results given the same input. When defining metrics in an academic paper, authors sometimes omit explicit formulations or give references that do not contain explanations either. In 47\% of cases, we cannot easily know how the metric is defined because the definition is not clear or absent. These findings highlight yet another difficulty in recommender system evaluation and call for a more detailed description of evaluation protocols.},
  keywords = {LensKit References,offline evaluation recommender systems metrics}
}

@inproceedings{trec5-confusion,
  title = {Report on the {{TREC-5 Confusion Track}}},
  booktitle = {The {{Fifth Text REtrieval Conference}} ({{TREC-5}})},
  author = {Kantor, Paul B and Voorhees, Ellen},
  year = {1997},
  month = oct,
  url = {http://trec.nist.gov/pubs/trec5/t5_proceedings.html},
  keywords = {CAREER,LensKit References}
}

@inproceedings{vargasRankRelevanceNovelty2011,
  title = {Rank and {{Relevance}} in {{Novelty}} and {{Diversity Metrics}} for {{Recommender Systems}}},
  booktitle = {{{RecSys}} '11},
  author = {Vargas, Sa{\'u}l and Castells, Pablo},
  year = {2011},
  pages = {109--116},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2043932.2043955},
  urldate = {2014-05-03},
  abstract = {The Recommender Systems community is paying increasing attention to novelty and diversity as key qualities beyond accuracy in real recommendation scenarios. Despite the raise of interest and work on the topic in recent years, we find that a clear common methodological and conceptual ground for the evaluation of these dimensions is still to be consolidated. Different evaluation metrics have been reported in the literature but the precise relation, distinction or equivalence between them has not been explicitly studied. Furthermore, the metrics reported so far miss important properties such as taking into consideration the ranking of recommended items, or whether items are relevant or not, when assessing the novelty and diversity of recommendations. We present a formal framework for the definition of novelty and diversity metrics that unifies and generalizes several state of the art metrics. We identify three essential ground concepts at the roots of novelty and diversity: choice, discovery and relevance, upon which the framework is built. Item rank and relevance are introduced through a probabilistic recommendation browsing model, building upon the same three basic concepts. Based on the combination of ground elements, and the assumptions of the browsing model, different metrics and variants unfold. We report experimental observations which validate and illustrate the properties of the proposed metrics.}
}

@misc{vieira2014gumbel,
  title = {Gumbel-Max Trick and Weighted Reservoir Sampling},
  author = {Vieira, Tim},
  year = {2014},
  url = {http://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/}
}

@misc{vieira2019swor-algs,
  title = {Algorithms for Sampling without Replacement},
  author = {Vieira, Tim},
  year = {2019},
  url = {https://timvieira.github.io/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/}
}

@article{vigTagGenomeEncoding2012,
  title = {The {{Tag Genome}}: {{Encoding Community Knowledge}} to {{Support Novel Interaction}}},
  author = {Vig, Jesse and Sen, Shilad and Riedl, John},
  year = {2012},
  month = sep,
  journal = {ACM Trans. Interact. Intell. Syst.},
  volume = {2},
  number = {3},
  pages = {13:1--13:44},
  issn = {2160-6455},
  doi = {10.1145/2362394.2362395},
  urldate = {2014-05-02},
  abstract = {This article introduces the tag genome, a data structure that extends the traditional tagging model to provide enhanced forms of user interaction. Just as a biological genome encodes an organism based on a sequence of genes, the tag genome encodes an item in an information space based on its relationship to a common set of tags. We present a machine learning approach for computing the tag genome, and we evaluate several learning models on a ground truth dataset provided by users. We describe an application of the tag genome called Movie Tuner which enables users to navigate from one item to nearby items along dimensions represented by tags. We present the results of a 7-week field trial of 2,531 users of Movie Tuner and a survey evaluating users' subjective experience. Finally, we outline the broader space of applications of the tag genome.}
}

@article{virtanenSciPyFundamentalAlgorithms2020,
  title = {{{SciPy}} 1.0: Fundamental Algorithms for Scientific Computing in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J and Brett, Matthew and Wilson, Joshua and Millman, K Jarrod and Mayorov, Nikolay and Nelson, Andrew R J and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E A and Harris, Charles R and Archibald, Anne M and Ribeiro, Ant{\^o}nio H and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  year = {2020},
  month = mar,
  journal = {Nat. Methods},
  volume = {17},
  number = {3},
  pages = {261--272},
  issn = {1548-7091},
  doi = {10.1038/s41592-019-0686-2},
  abstract = {SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments.},
  keywords = {LensKit References,Software}
}

@inproceedings{yagciParallelizingSGDPairwise2017,
  title = {On {{Parallelizing SGD}} for {{Pairwise Learning}} to {{Rank}} in {{Collaborative Filtering Recommender Systems}}},
  booktitle = {Proceedings of the {{Eleventh ACM Conference}} on {{Recommender Systems}}},
  author = {Yagci, Murat and Aytekin, Tevfik and Gurgen, Fikret},
  year = {2017},
  series = {{{RecSys}} '17},
  pages = {37--41},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/3109859.3109906},
  urldate = {2019-05-17},
  abstract = {Learning to rank with pairwise loss functions has been found useful in collaborative filtering recommender systems. At web scale, the optimization is often based on matrix factorization with stochastic gradient descent (SGD) which has a sequential nature. We investigate two different shared memory lock-free parallel SGD schemes based on block partitioning and no partitioning for use with pairwise loss functions. To speed up convergence to a solution, we extrapolate simple practical algorithms from their application to pointwise learning to rank. Experimental results show that the proposed algorithms are quite useful regarding their ranking ability and speedup patterns in comparison to their sequential counterpart.},
  isbn = {978-1-4503-4652-8}
}

@article{yaoMeasuringRetrievalEffectiveness1995,
  title = {Measuring {{Retrieval Effectiveness Based}} on {{User Preference}} of {{Documents}}},
  author = {Yao, Y Y},
  year = {1995},
  journal = {Journal of the American Society for Information Science},
  volume = {46},
  number = {2},
  pages = {133--145},
  issn = {0002-8231},
  urldate = {2015-11-05},
  abstract = {Discusses user preferences for the representation, interpretation, and measurement of the relevance or usefulness of retrieved documents. Highlights include measurement of user judgments on documents; distance between rankings; measures of retrieval effectiveness; normalized performance measure; and the relationship of distance-based measures to other performance measures. (53 references) (LRW)},
  keywords = {LensKit References,metrics}
}

@inproceedings{zhangConferConferenceRecommendation2016,
  title = {Confer: {{A Conference Recommendation}} and {{Meetup Tool}}},
  booktitle = {Proceedings of the 19th {{ACM Conference}} on {{Computer Supported Cooperative Work}} and {{Social Computing Companion}}},
  author = {Zhang, Amy X. and Bhardwaj, Anant and Karger, David},
  year = {2016},
  series = {{{CSCW}} '16 {{Companion}}},
  pages = {118--121},
  publisher = {ACM},
  address = {New York, NY, USA},
  doi = {10.1145/2818052.2874340},
  isbn = {978-1-4503-3950-6},
  keywords = {collaborative filtering,conference planning,match-making,recommendation}
}

@inproceedings{zhouLargeScaleParallelCollaborative2008,
  title = {Large-{{Scale Parallel Collaborative Filtering}} for the {{Netflix Prize}}},
  booktitle = {Algorithmic {{Aspects}} in {{Information}} and {{Management}}},
  author = {Zhou, Yunhong and Wilkinson, Dennis and Schreiber, Robert and Pan, Rong},
  year = {2008},
  pages = {337--348},
  publisher = {Springer Berlin Heidelberg},
  doi = {10.1007/978-3-540-68880-8_32},
  abstract = {Many recommendation systems suggest items to users by utilizing the techniques of collaborative filtering (CF) based on historical records of items that the users have viewed, purchased, or rated. Two major problems that most CF approaches have to contend with are scalability and sparseness of the user profiles. To tackle these issues, in this paper, we describe a CF algorithm alternating-least-squares with weighted-{$\lambda$}-regularization (ALS-WR), which is implemented on a parallel Matlab platform. We show empirically that the performance of ALS-WR (in terms of root mean squared error (RMSE)) monotonically improves with both the number of features and the number of ALS iterations. We applied the ALS-WR algorithm on a large-scale CF problem, the Netflix Challenge, with 1000 hidden features and obtained a RMSE score of 0.8985, which is one of the best results based on a pure method. In addition, combining with the parallel version of other known methods, we achieved a performance improvement of 5.91\% over Netflix's own CineMatch recommendation system. Our method is simple and scales well to very large datasets.},
  keywords = {LensKit References}
}
